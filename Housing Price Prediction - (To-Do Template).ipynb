{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# California Housing Price Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "db00efbb-83bf-4291-9d03-806e04fb172e",
    "_uuid": "bf0a1a372d8b3539b3f5b64b6160de862cdba2c3"
   },
   "source": [
    "> ##  Business Problem: \n",
    "\n",
    "To predict the prices of houses in Californa based on their different specifications and locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ##   Description : \n",
    "\n",
    "The Dataset is built using the 1990 California census data. It contains one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people).\n",
    "\n",
    "The information was collected on the variables using all the block groups in California from the 1990 Census. In this sample a block group on average includes 1425.5 individuals living in a geographically compact area. Naturally, the geographical area included varies inversely with the population density. Distances were computed among the centroids of each block group as measured in latitude and longitude and all the districts reporting zero entries for the independent and dependent variables were excluded. The final data contained 20,640 observations on 9 variables. The dependent variable is ln(median house value). The other variables are as follows: \n",
    "\n",
    "    1. longitude: A measure of how far west a house is; a higher value is farther west\n",
    "    2. latitude: A measure of how far north a house is; a higher value is farther north\n",
    "    3. housingMedianAge: Median age of a house within a block; a lower number is a newer building\n",
    "    4. totalRooms: Total number of rooms within a block\n",
    "    5. totalBedrooms: Total number of bedrooms within a block\n",
    "    6. population: Total number of people residing within a block\n",
    "    7. households: Total number of households, a group of people residing within a home unit, for a block\n",
    "    8. medianIncome: Median income for households within a block of houses (measured in tens of thousands of US Dollars)\n",
    "    9. medianHouseValue: Median house value for households within a block (measured in US Dollars)\n",
    "    10. oceanProximity: Location of the house w.r.t ocean/sea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Approach : \n",
    "\n",
    "You will understand the data and make the best model in the following stages : \n",
    "\n",
    "- **Data Description**\n",
    "\n",
    "- **Exploratory Data Analysis**\n",
    "         \n",
    "      - Uni-Variate Analysis : Boxplot , Histogram , Barplot\n",
    "      - Correlation Analysis : Correlation Matrix\n",
    "      - Bi-Variate Analysis : Scatter Matrix and Plot\n",
    "      - Multi-Variate Analysis : Scatter Plot\n",
    "      \n",
    "- **Data Cleaning and Manipulation**\n",
    "    \n",
    "      - Attribute Combination\n",
    "      - OneHotEncoding Categorical Attributes\n",
    "      - Missing Values Handling\n",
    "\n",
    "- **Data Sampling and Splitting**\n",
    "     \n",
    "      - Stratified Sampling : Implementation and Comparison\n",
    "      - Train-Validation-Test Splitting\n",
    "\n",
    "- **Modelling and CrossValidating**\n",
    "\n",
    "      - Linear Regression \n",
    "      - Decision Tree \n",
    "      - Random Forest \n",
    "\n",
    "- **HyperTuning : GridSearch** \n",
    "- **Test Set Evaluation**\n",
    "- **Conclusion** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `1` Data Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "#importing Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# loading data\n",
    "data_path = 'C:/Users/kusht/OneDrive/Desktop/Excel-csv/housing.csv'\n",
    "housing = pd.read_csv(data_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Get info about the dataset using `info()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE (~ 1 Line of code)\n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b49f8321-dc46-4b76-9a75-3e5e6a51f1d5",
    "_uuid": "46853c813bed59faa5898515448809318199c8bc"
   },
   "source": [
    "**TASK : Fill the information we get from `info()`**\n",
    "- total observations: `_`  (Each observation is the data about a block group)\n",
    "- total columns (features): `_`\n",
    "- data type of each feature: `_` numberical and `_` object \n",
    "- features with null values : `____` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1f1c703b-8bcb-4de8-8a0b-a8417f632a5a",
    "_uuid": "630644f793ed0e3f9297ae494539a727c24f1ab1",
    "scrolled": true
   },
   "source": [
    "**TASK : Output the first five instances of the dataset and analyse**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE (~1 Line of code)\n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6880a522-94d7-409e-8234-9fffa393b285",
    "_uuid": "cfb563121e2ca78709203b9c6cbbfca237e88ac7"
   },
   "source": [
    "`describe()` shows a summary of **Numerical Features** , which can be visualized using boxplots and histograms. `value_counts()` can be used to generate a summary of **Categorical Attributes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Describe the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "c28f2ea3-7735-458c-a4ca-749d751da239",
    "_uuid": "6b6c05c0400ec70bc5f7e82337dbbed4690507f5"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE (~ 1 Line of code)\n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `2`  Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `2.1` Uni-variate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Make a `Boxplot` of `median_house_value`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "44ff7bf4-6c17-46a6-8d9e-af0684836059",
    "_uuid": "d93c407c87049bcace672abc8b6cad5fbc27878e"
   },
   "outputs": [],
   "source": [
    "### Create BOXPLOT using boxplot() and keep figsize=(6,6)\n",
    "\n",
    "### START CODE HERE : (~ 1 Line of code)\n",
    "\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Now make `histograms` of all of the Numerical Attributes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "d5b4ed7d-f927-4f6b-af8b-40121ad35b34",
    "_uuid": "fb3a2350ccad322f62e33e1fab160f358b104b67"
   },
   "outputs": [],
   "source": [
    "# Create histograms of all attributes in one line using hist() function and keep figsize=(15,15)\n",
    "\n",
    "### START CODE HERE : (~1 Line of code)\n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d31a4f47-a2f1-40b0-8f60-c8c7ff80c6e8",
    "_uuid": "da7ee409c9f8fbd379d70bd54abcf5fc790554e8"
   },
   "source": [
    "\n",
    "Given that `.boxplot()` and `.hist()` only handle numerical features. You have to use other technique to visualise categorical attributes like `ocean_proximity`, which is object type "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One idea is to plot a bar graph between elements names/labels and their respective counts/frequencies . The frequencies can be found out using `value_counts()` method and a barplot can be plotted between indexes and values of the categorical attributes value_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task : Find value counts of the categorical attribute and store it in a variable `op_count`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "3647db42-ddd2-40aa-aa04-da7f38c77f36",
    "_uuid": "d9a3a52af4d4449ef1ac66656cb73cc7795bdf9c"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE (~ 1 Line of code)\n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Plot a `bar graph` between op_count indexes and op_count values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "5dc65421-e679-4cf5-8d7b-57d8eec7761b",
    "_uuid": "cb15a204d73352978a9e98f32b8e09fdc6b4ca5a"
   },
   "outputs": [],
   "source": [
    "### Parameters: figsize=(10,5) , alpha= 0.7 , fontsize=12 for x and y labels\n",
    "\n",
    "### START CODE HERE (FULL CODE)\n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ababde1f-a4e1-40e7-a089-e63934e19c36",
    "_uuid": "5a0fcc70b05cf4e11f83dbcef8c5353b010d37b2"
   },
   "source": [
    "#### Understand and Analyse The Data\n",
    "1. Make sense of the data\n",
    "\n",
    "      - **Why are total rooms and bedrooms in hunderds or thousands?** \n",
    "      - **Is population in thousands or millions or just the number of people living in that block?** \n",
    "      - **Why does median income value is so low? is it already scaled?** \n",
    " \n",
    "\n",
    "2. **Feature Scaling** : Is it required? \n",
    " \n",
    "3. **Distribution** : from the histograms what can you infer , is the data skewed or normal? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2ec535a8-89d0-42a2-b960-477d653131f2",
    "_uuid": "7b4bea83faa759761430bac0f3d4a5cf52212f63"
   },
   "source": [
    "### `2.2`  Correlation Analysis\n",
    "Further\n",
    "explore the data to look for correlations between different attributes. correlation coefficient is between -1 and 1, representing negative and positive correlations. 0 means there is no liner correlation. Correlation is said to be linear if the **ratio of change** is constant, otherwise is non-linear. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Print the `Correlation Matrix` and make it more visually appealing using `sns.heatmap`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "4af21b33-b350-420e-af52-f328c2bfbe99",
    "_uuid": "3b2fb3c1f0a3ea1443f49f7de2d7463b0a97c986"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE (FULL CODE)\n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `2.3` Bi-Variate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scatter matrix gives a good idea about histograms of each attribute and their dependencies on each other through scatter plots between each pair**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Plot a `scatter matrix`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "cdfbb263-843b-4dac-b448-26f69f97a5fd",
    "_uuid": "cb1a1f8c3af889e45a3bdb66a5370c008c34411f"
   },
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "### Plot scatter matrix only of attrbutes : 'median_house_value', 'median_income', 'total_rooms', 'housing_median_age'\n",
    "### Keep figsize = (12,12)\n",
    "\n",
    "### START CODE HERE \n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5cfc8d48-b50a-40e2-948c-436cf7fbfd03",
    "_uuid": "037e979a849d82a6239a2cf855e2c3932167374d",
    "collapsed": true
   },
   "source": [
    "Analyse the promising attributes by seeing which forms the closest linear plot and judging values of correlation matrix with respect to `median_house_value`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Note** that in scatter plots , like between `median_income` and `median_house_value` , there are many horizontal lines which are abrupting the linearity between the two attributes . \n",
    "**Think of a reason for this and you can remove them which might help in better model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Plot an individual scatter plot between `median_income` and `median_house_value` to understand the problem better**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Keep figsize=(10,10) and alpha=0.2\n",
    "\n",
    "### START CODE HERE (FULL CODE)\n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK (Optional but effective) : Remove the duplicate values of `median_house_value`** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **HINT 1** : Try to set a criterion for removing , like removing instances if their number of duplicates are greater than 20 or any number of your choice. you can use value_counts to see which threshold number would be better. This way information of attributes will be retained as there are still 20 duplicates and yet plot would be more linear as again , there's only 20 duplicates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **HINT 2 (Basic Approach)** : You can find all the instances in `median_house_value` having duplicates >20 using `duplicated()` and `value_counts()` and then remove those instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### You can use as many functions , variables and for loops you want but try to find the most optimum code\n",
    "\n",
    "### START CODE HERE ( FULL CODE )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TASK (Optional) : Plot the scatter plot and compare the results with previous plot** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE (~2 Lines of code)\n",
    "\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyse the Scatter plots and decide whether it has been a useful exercise or not**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `2.4` Multi-Variate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get to know the shape and see your dataset covers mostly all parts of california , a scatter plot of latitude and longitude can be especially important to know the densities at each point in california"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Plot a scatter plot between `Latitude` and `Longitude` and analyse**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use alpha = 0.1 \n",
    "\n",
    "### START CODE HERE : (~ 1 Line of code)\n",
    "\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Does it look like the map of California?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://ca-at.org/wp-content/uploads/2016/04/County-Map-Colored.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract more information from it you can also plot a `Multi-Variate Scatter plot` with size as the `housing population` and color as `meidan_house_Value`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Extract more information from scatter plot by plotting `latitude` , `longitude` , `median_house_value` and `population` in the same scatter plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  s: radius of each circle represent the housing.population/100\n",
    "#  c: Put meidan_house_value as the color scheme\n",
    "\n",
    "### START CODE HERE : (WRITE CODE WHERE '#' IS GIVEN)\n",
    "\n",
    "housing.plot(kind='#' , x='#', y='#', alpha=0.4, \n",
    "    s='#', label='population', figsize=(10,7), \n",
    "    c='#' , cmap=plt.get_cmap('jet'), colorbar=True)\n",
    "\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also extract more information in the same way using `s` and `c` parameters as different attributes like ocean_proximity etc to get more insight about the data . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This completes your visualisation and one should always analyse the graphs to get a better understanding of how and what type of model to build or data to manipulate to get best results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `3` Data Cleaning and Manipulation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you have to clean the data and make new chnages/manipulations before models are build on it "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1b82d9b-73ad-463c-83aa-6a7e00971d35",
    "_uuid": "c0742bf6798c7fc3f30ebfe3722b56585c8f3fe0"
   },
   "source": [
    "### `3.1` Attribute Combinations\n",
    "While total rooms and bedrooms might not be that useful however a combination of attributes can help to create a much more meaningful attribute for eg : \n",
    "\n",
    "- rooms per household\n",
    "- bedroom/total room ratio\n",
    "- population per household\n",
    "\n",
    "These are just a few , you can think of more such attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Create new attributes by combining previous ones and check out correlation again**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7a94f687-7045-4688-a529-549d567900ec",
    "_uuid": "b12a658bc9002f2f6880a1b2eb64b28b7b260a30"
   },
   "outputs": [],
   "source": [
    "### You can create any choice of new attrbute but it should make sense\n",
    "### For now lets stick to the new attribute cominations that are mentioned above\n",
    "\n",
    "### START CODE HERE : (WRITE CODE WHERE '#' IS GIVEN)\n",
    "\n",
    "# Calculated attributes : \n",
    "\n",
    "housing['rooms_per_household'] = housing['#']/housing['#']\n",
    "    # Write attribute name which forms the combination\n",
    "\n",
    "# Write code for bedroom/total room and population per household\n",
    "\n",
    "# Checkout the correlations again\n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3fd2032b-b4f5-4029-bb27-3159da0eaa9e",
    "_uuid": "cabd99787ffe46b29f3434e571b5fdded3d58a6d"
   },
   "source": [
    "**TASK : Analyse the new attributes correlations with others and judge if it was a good decision to form these attributes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE (FULL CODE)\n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b2d65178-66dd-4ce8-a674-ab6ef8a5e4d6",
    "_uuid": "55b8d5f52fd86a9f8451ca4c7faad4f0e3aba660"
   },
   "source": [
    "### `3.2` Text and Categorial Attributes\n",
    "Most ML algorithms and visualisations work with numbers better. Therefore, you need to convert text attributes into numerical attributes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Convert categorical attribute to numerical attribute**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use LabelEncoder() , Binarizer() , OneHotEncode() or get_dummies whichever seems the mose convenient to you. Decide and analyse whether its safe to just label encode it or does it require one hot encoding then encode it via any of the functions method above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_cell_guid": "c7b00738-dbf6-429c-8a54-51f835e315ab",
    "_uuid": "6980028b8310cc298517265f4942c78e4a4b7c9f"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE : (FULL CODE)\n",
    "\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `3.3` Missing Values \n",
    "\n",
    "Now missing values in some attributes need to be handled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Use `info()` method to know which attributes has missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE (~1 Line of code)\n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Use `fillna()` method to fill the missing values with `mean` and check `info()` again for confirmation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE (~ 2 Lines of code)\n",
    "\n",
    "\n",
    "# END CODE HERE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `4` Data Sampling and Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section would guide you to choose the best option of sampling seeing the dataset and then implementing it by creating training , validation and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `4.1` Stratified Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statistics, stratified sampling is a method of sampling from a population which can be partitioned into subpopulations.\n",
    "An exmaple to explain better would be to assume that we need to estimate the average number of votes for each candidate in an election. Assume that a country has 3 towns: Town A has 1 million factory workers, Town B has 2 million office workers and Town C has 3 million retirees. We can choose to get a random sample of size 60 over the entire population but there is some chance that the resulting random sample is poorly balanced across these towns and hence is biased, causing a significant error in estimation. Instead if we choose to take a random sample of 10, 20 and 30 from Town A, B and C respectively, then we can produce a smaller error in estimation for the same total sample size. This method is generally used when a population is not a homogeneous group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Stratified_sampling.PNG/220px-Stratified_sampling.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.researchgate.net/profile/Stefano_Ferilli/publication/216799623/figure/fig3/AS:650409278468108@1532081064594/Stratified-sampling-methodology.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First understand why straitified sampling is crucial here and then analyse which attribute you can base your sampling on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A good option would be to stratify sample it on basis of `income` . Lets try it out "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Stratify sample on basis of `median_income`** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the documentation and examples of stratified sampling [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "limit the number of categories by dividing the median income by 1.5 and merge all the income greater than 5 into 5. Then, you can use stratified sampling. This is done to create bins and small range for sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Task : Use `np.ceil` to create bins and divide `median_income` by 1.5 to create small convenient range**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE : (WRITE CODE WHERE '#' IS GIVEN)\n",
    "\n",
    "housing['income_cat'] = np.ceil(housing['#']/1.5) \n",
    "# Write the attributes name\n",
    "\n",
    "# Merge value greater than 5 to 5 in the same variable 'income_cat'\n",
    "# Print a histogram of 'income_cat' to see the distribution \n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Task : Create an object `split` of `StratifiedShuffleSplit()` and use it to create Stratified train and test sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratified sampling based on income categories\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "### START CODE HERE : (WRITE CODE IN PLACE OF '#')\n",
    "\n",
    "split = StratifiedShuffleSplit('#')\n",
    "# Use n_splits as 1 , test size as 0.2 and random state = 42\n",
    "\n",
    "## Create Stratified Train and Test Sets\n",
    "\n",
    "## The following function will split into stratified samples and return the train_index and test_index of samples\n",
    "for train_index, test_index in split.split(housing, housing['#']):\n",
    "    # Write the attribute name on basis of which sampling is done\n",
    "    \n",
    "    strat_train_set = # Fill this using loc() and train_index given dataset housing\n",
    "    strat_test_set = #  Fill this using loc() and test_index given dataset housing\n",
    "\n",
    "# Print the first five elements of strat_train_set\n",
    "\n",
    "### END CODE \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can check that stratified sampling distribution is much more similar to the original distribution than random sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TASK : Check distributions of stratified sampling and random sampling and compare it with original distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Straitified sampling\n",
    "\n",
    "### START CODE HERE (WRITE CODE WHERE '#' IS GIVEN)\n",
    "\n",
    "#['income_cat'].value_counts() / len(#)\n",
    "\n",
    "## Replace # with strat_train_set to know the distribution of stratified sampling\n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Original sampling\n",
    "\n",
    "### START CODE HERE (WRITE CODE WHERE '#' IS GIVEN)\n",
    "\n",
    "#['income_cat'].value_counts() / len(#)\n",
    "\n",
    "## Replace # with original datafram to know the original distribution \n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Random sampling \n",
    "\n",
    "### START CODE HERE (FULL CODE)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Write code to get random split samples from train_test_split() and store it in train_set and test_set\n",
    "\n",
    "#['income_cat'].value_counts() / len(#)\n",
    "\n",
    "## Replace # with train_set to know the distribution of random sampling\n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Remove the income_cat variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE (FULL CODE)\n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyse and decide which sampling to take and thus that sampling's training set would be your `current training set` henceforth. Keep the chosen `sampling's test set aside` as no changes or modelling should be done on that , that test set would only be used in the last evaluation section when the best model has already been built** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `4.2`  Create training and validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Split your `current training set` into dependent and independent variables and name them y and X respectively**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE (~ 2 Lines of code)\n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Use `train_test_split` to split the X and y into X_train , X_val , y_train , y_val**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Keep the test_size=0.2 and random_state=42\n",
    "\n",
    "### START CODE HERE (FULL CODE)\n",
    "\n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** : You're creating a train set and test set from the sampling you choose. Then you keep the test set aside and split the train set into actual training set and validation set. All models would be trained on this training set and evaluated on the validation set. Once youre convinced that you have the best model , then you finally test it on the initial test set that was kept aside. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "932afe85-ebd9-44f6-90cd-d5372ebaddd2",
    "_uuid": "62a100e0cac4fb655d8efe3ad934e7c19e18259e"
   },
   "source": [
    "## `5` Modelling and Cross-Validating\n",
    "You are going to try Linear Regression, Decision Tree, Random Forest models and cross validate them to analyse the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `5.1` Linear Regression : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple linear regression is useful for finding relationship between two continuous variables. One is predictor or independent variable and other is response or dependent variable. It looks for statistical relationship but not deterministic relationship. Relationship between two variables is said to be deterministic if one variable can be accurately expressed by the other. For example, using temperature in degree Celsius it is possible to accurately predict Fahrenheit. Statistical relationship is not accurate in determining relationship between two variables. For example, relationship between height and weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/4328/1*KwdVLH5e_P9h8hEzeIPnTg.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core idea is to obtain a line that best fits the data. The best fit line is the one for which total prediction error (all data points) are as small as possible. Error is the distance between the point to the regression line. \n",
    "Any line can be represented as `y = (theta)X + b` , where theta is the slope or weight and b is a constant whereas y is our predicted value of the target variable and x is our training set . The goal is to minimize the difference between `y_pred` and `y` (origial values of the target variable) by finding the best `weights` . Its determined by the formula : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://patientopinioncorpus.files.wordpress.com/2013/12/normal-equation.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where X and y are represented as vectors. The can be implemented through manual coding but you can use built in libraries and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Build a linear regression model,fit on the training set and print the rmse (`Root Mean Squared Error`)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "_cell_guid": "c9afd3fc-b59b-421c-ba63-e6594629f02c",
    "_uuid": "42c3f81ece6a53b5f251dd97f52bbe3d03e1e6b9"
   },
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "### START CODE (FULL CODE)\n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Describe the training set and get its `25%` and `75%`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b9a4162a-b978-465b-b889-a61789e2dfa5",
    "_uuid": "0bf417daca5e1399299fbae480d831de9b073b51"
   },
   "source": [
    "Analyse its `25%` and `75%` quantile and judge whether the RMSE is good or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `5.2` Decision Tree :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree can be used to visually and explicitly represent decisions and decision making. As the name goes, it uses a tree-like model of decisions. It chooses a path based on the decisions and ultimately ends in a classification or regression figure. It can be understood better with a diagram : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://dimensionless.in/wp-content/uploads/2018/11/Picture1-1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression trees are represented in the same manner, just they predict continuous values like price of a house."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Use built in libaries to implement `Decision Trees` , fit it on the training set and print the `RMSE` values on the training set predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "_cell_guid": "811ed0a0-2674-4204-8016-6def39f46a17",
    "_uuid": "a3867c9d2347e7bf4cc825d037bcadd44c0583f3"
   },
   "outputs": [],
   "source": [
    "# Try Decision Tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "### START CODE HERE (FULL CODE)\n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "64436baf-c382-49e1-a780-c08e27fd11d7",
    "_uuid": "51ad5326a7657d30e8980f69d8e096eb0c7cd920"
   },
   "source": [
    "**Does this mean `Decision Trees` gives the best model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse this and convince yourself about the reason for such a value. Check whether its really a great model using `cross validation`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation is primarily used in applied machine learning to estimate the skill of a machine learning model on unseen data. That is, to use a limited sample in order to estimate how the model is expected to perform in general when used to make predictions on data not used during the training of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general procedure is as follows:\n",
    "\n",
    "    1. Shuffle the dataset randomly.\n",
    "    2. Split the dataset into k groups\n",
    "    3. For each unique group:\n",
    "        \n",
    "        - Take the group as a hold out or test data set\n",
    "        - Take the remaining groups as a training data set\n",
    "        - Fit a model on the training set and evaluate it on the test set\n",
    "        - Retain the evaluation score and discard the model\n",
    "\n",
    "    4. Summarize the skill of the model using the sample of model evaluation scores\n",
    "\n",
    "Importantly, each observation in the data sample is assigned to an individual group and stays in that group for the duration of the procedure. This means that each sample is given the opportunity to be used in the hold out set 1 time and used to train the model k-1 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Get `Mean RMSE` cross validation scores for `Decision Tree` and `Linear Regression Model`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "_cell_guid": "7dd05ddd-a709-4a79-8920-df32a8400645",
    "_uuid": "1421162c51bd6bcb15e05e93b8c364c06b1b27b9"
   },
   "outputs": [],
   "source": [
    "# Do a 5-fold cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "### START CODE HERE : (FULL CODE)\n",
    "\n",
    "# for decision tree\n",
    "\n",
    "\n",
    "# for linear regression\n",
    "\n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b32fe0fb-84fe-488d-8830-f33913fa2bc2",
    "_uuid": "9bc97deb1e0a5adfe816317b725aa6b10e537ee2"
   },
   "source": [
    "**Is the Decision Tree model still better?** \n",
    "Analyse whether `RMSE` values are even good for both Decision Tree or Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To further improve the `RMSE` , you should now try different algorithms like `Random Forest`** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `5.3` Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you already have an idea baout Decision Trees , It's easier to understand `Random Forest` . Random forest, like its name implies, consists of a large number of individual decision trees that operate as an [ensemble](https://en.wikipedia.org/wiki/Ensemble_learning) . Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model’s prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1000/1*VHDtVaDPNepRglIAv72BFg.jpeg\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fundamental concept is  large number of relatively uncorrelated models (trees) operating as a committee will outperform any of the individual constituent models. Since this dataset has very low correlation between attributes , random forest can be a good option. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Implement `Random Forest` from built in Libraries , fit on training set and print `Mean RMSE Cross Validation` score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "_cell_guid": "266b8bfe-e277-44a7-8d3c-3e6c77101ce9",
    "_uuid": "e97f302c4ab871ce359cb98ab6d8ea63c8ae5372"
   },
   "outputs": [],
   "source": [
    "# Try Random Forest, which is an Ensemble Learning model\n",
    "# Use CrossValidation K-fold = 5\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "### START CODE HERE (FULL CODE)\n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse and compare this model with the previous two models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your modelling is done and its time to **hypertune** the best model's parameters to further decrease the `RMSE` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "13e78f06-3e6e-44b2-a26c-81f4a30e1318",
    "_uuid": "f96ed0fefb8873d46217f5b3c936cf91e6e1f2b9"
   },
   "source": [
    "## `6` HyperTuning\n",
    "\n",
    "Once you've chosen the best model , look for its documentation , see its parameters and make an appropriate `param_grid` and then do a GridSearch to find the best combinations of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you'll learn how to implement hypertuning of random forest model's parameters and you can hypertune other models based on that  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Hypertuning : GridSearch\n",
    "\n",
    "Grid-search is used to find the optimal hyperparameters of a model which results in the most ‘accurate’ predictions. It builds a model for every combination of hyperparameters specified and evaluates each model. A more efficient technique for hyperparameter tuning is the Randomized search — where random combinations of the hyperparameters are used to find the best solution. However , if its a small sample like the current dataset then gridsearch is also fine. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Form param_grid and do a `GridSearch` to print the best parameters for `Random Forest` model** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "_cell_guid": "0ff1d832-23b4-4d28-9170-56a117632c1d",
    "_uuid": "bedc716a898798c0e561ef11a873778634fec17d"
   },
   "outputs": [],
   "source": [
    "# use GridSearch to find best hyperparameter combinations\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "## You can use a standard  param_grid as : \n",
    "## {n_estimators : [3,10,30] , max_features : [2,4,6,8]},{bootstrap : [False] , n_estimators:[3,10]} , max_features : [2,3,4]}\n",
    "## you can also use your own param_grid \n",
    "## cross validation k-fold =5 \n",
    "\n",
    "###  START CODE HERE : (FULL CODE)\n",
    "\n",
    "\n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Print the best estimator** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "_cell_guid": "b686f35f-1bb3-4b38-b235-d1cee88de630",
    "_uuid": "b8a176643b9d7590d0e5bab26a6de72a8de366d0"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE (~ 1 Line of code)\n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Print the feature importance** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "_cell_guid": "445493b3-92ea-45ca-a847-0f481336d220",
    "_uuid": "1aef221e5f179796989ea9d1738012637eebddb7"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE (~ 1 Line of code)\n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Print the attributes along with their feature importance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HINT** : Try using zip() method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b2cfeb2e-4d00-47f5-b0a5-296b61f14bc4",
    "_uuid": "d960681098d397773bdb199f012322145ffc7d4a"
   },
   "source": [
    "Based on the feature importance, you can choose to drop some features such as the last four/least four important fetaures to simplify the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Predict the values of the validation set which we saved as X_val , y_val for all the three models and analyse which is the best** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_HyperTuned_model = grid_search.best_estimator_\n",
    "\n",
    "### START CODE HERE (FULL CODE)\n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyse the scores and pick out the best one**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelling and Hypertuning is done and the only thing left to do is to try it on the **Test Set**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "42d2431e-49f7-4b8f-aef6-3490cbd8625b",
    "_uuid": "15689a515c1c8173463663978cba6099e50bcac2"
   },
   "source": [
    "## `7` Evaluation via the Test Set\n",
    "This step is to see how the model performs on unknow data. As long as the result is not way off from the validation result, you should go ahead lauch the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Split the earlier calculated `strat_test_set` into dependent and independent variables and name them y_test and X_test respectively**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "_cell_guid": "833f4fd2-d806-4369-956c-2530ef606837",
    "_uuid": "9b77dd26834654d0dafd30287e16de815af3b1f9"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE (FULL CODE)\n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**  : Check for missing values and remove them if any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK: Calculate the `RMSE` Values for the best model on the test set** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE (FULL CODE)\n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "77ea9ea6-237d-4530-be01-792ae0e04328",
    "_uuid": "c085c2587b3b21f76d9ea595738b6de227de5d07"
   },
   "source": [
    "### Discussion and Conclusion:  \n",
    "\n",
    "Find the `25 %` and `75 %` Quantile and see if the RMSE obtained from the best model is adequate or not. Also try out other model and come to a conclusive best model and write down its characterstics :\n",
    "\n",
    "- **Model Algorithm** : `____`\n",
    "- **Model Parameters** : `____`\n",
    "- **Root Mean Squared Error** : `___`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
